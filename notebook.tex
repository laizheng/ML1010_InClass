
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{1\_feature\_engineering}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \hypertarget{text-data---natural-language-processing-nlp}{%
\section{Text Data - Natural Language Processing
(NLP)}\label{text-data---natural-language-processing-nlp}}

    \hypertarget{section}{%
\section{===================================}\label{section}}

\hypertarget{part-a.-introductory-materials}{%
\section{Part A. Introductory
Materials}\label{part-a.-introductory-materials}}

\hypertarget{section-1}{%
\section{===================================}\label{section-1}}

    Text data usually consists of a collection of documents (called the
corpus) which can represent words, sentences, or even paragraphs of free
flowing text.

The inherent unstructured (no neatly formatted data columns!) and noisy
nature of textual data makes it harder for machine learning methods to
directly work on raw text data.

    \hypertarget{feature-engineering}{%
\section{Feature Engineering}\label{feature-engineering}}

    Feature engineering dramatically improve performance of machine learning
models and wins Kaggle competitions. This is especially true for text
data, which is unstructured, noisy, and complex.

This section will cover the following types of features for text data

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Bag of Words
\item
  Bag of N-Grams (uni-gram, bi-gram, tri-gram, etc.)
\item
  TF-IDF (term frequency over inverse document frequency)
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
        \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
        \PY{k+kn}{import} \PY{n+nn}{re}
        \PY{k+kn}{import} \PY{n+nn}{nltk}
        \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
\end{Verbatim}


    A sample ``corpus'' of documents: the Document contains short sentences
and each text belongs to a category.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{n}{corpus} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{The sky is blue and beautiful.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                  \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Love this blue and beautiful sky!}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                  \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{The quick brown fox jumps over the lazy dog.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                  \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{The brown fox is quick and the blue dog is lazy!}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                  \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{The sky is very blue and the sky is very beautiful today}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                  \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{The dog is lazy but the brown fox is quick!}\PY{l+s+s1}{\PYZsq{}}    
        \PY{p}{]}
        
        \PY{n}{labels} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{weather}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{weather}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{animals}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{animals}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{weather}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{animals}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
        \PY{n}{corpus} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{corpus}\PY{p}{)}
        \PY{n}{corpus\PYZus{}df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Document}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{corpus}\PY{p}{,} 
                                  \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Category}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{labels}\PY{p}{\PYZcb{}}\PY{p}{)}
        \PY{n}{corpus\PYZus{}df} \PY{o}{=} \PY{n}{corpus\PYZus{}df}\PY{p}{[}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Document}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Category}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{]}
        \PY{n}{corpus\PYZus{}df}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}2}]:}                                             Document Category
        0                     The sky is blue and beautiful.  weather
        1                  Love this blue and beautiful sky!  weather
        2       The quick brown fox jumps over the lazy dog.  animals
        3   The brown fox is quick and the blue dog is lazy!  animals
        4  The sky is very blue and the sky is very beaut{\ldots}  weather
        5        The dog is lazy but the brown fox is quick!  animals
\end{Verbatim}
            
    \hypertarget{text-pre-processing}{%
\section{Text pre-processing}\label{text-pre-processing}}

    Depending on your downstream task, cleaning and pre-processing text can
involve several different components. Here are a few important
components of Natural Language Processing (NLP) pipelines.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Removing tags: unnecessary content like HTML tags
\item
  Removing accented characters: other languages such as French, convert
  ASCII
\item
  Removing special characters: adds noise to text, use simple regular
  expressions (regexes)
\item
  Stemming and lemmatization: Stemming remove prefixes and suffixes of
  word stems (i.e.~root words), ex. WATCH is the root stem of WATCHES,
  WATCHING, and WATCHE. Lemmatization similar but lexicographically
  correct word (present in the dictionary).
\item
  Expanding contractions: helps text standardization, ex. do not to
  don't and I would to I'd
\item
  Removing stopwords: Words without meaningful significance (ex. a, an,
  the, and) but high frequency.
\end{enumerate}

Additional pre-processing: tokenization, removing extra whitespaces,
lower casing and more advanced operations like spelling corrections,
grammatical error corrections, removing repeated characters.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{n}{wpt} \PY{o}{=} \PY{n}{nltk}\PY{o}{.}\PY{n}{WordPunctTokenizer}\PY{p}{(}\PY{p}{)}
        \PY{n}{stop\PYZus{}words} \PY{o}{=} \PY{n}{nltk}\PY{o}{.}\PY{n}{corpus}\PY{o}{.}\PY{n}{stopwords}\PY{o}{.}\PY{n}{words}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{english}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        
        \PY{k}{def} \PY{n+nf}{normalize\PYZus{}document}\PY{p}{(}\PY{n}{doc}\PY{p}{)}\PY{p}{:}
            \PY{c+c1}{\PYZsh{} lower case and remove special characters\PYZbs{}whitespaces}
            \PY{n}{doc} \PY{o}{=} \PY{n}{re}\PY{o}{.}\PY{n}{sub}\PY{p}{(}\PY{l+s+sa}{r}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{[\PYZca{}a\PYZhy{}zA\PYZhy{}Z0\PYZhy{}9}\PY{l+s+s1}{\PYZbs{}}\PY{l+s+s1}{s]}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{doc}\PY{p}{,} \PY{n}{re}\PY{o}{.}\PY{n}{I}\PY{p}{)}
            \PY{n}{doc} \PY{o}{=} \PY{n}{doc}\PY{o}{.}\PY{n}{lower}\PY{p}{(}\PY{p}{)}
            \PY{n}{doc} \PY{o}{=} \PY{n}{doc}\PY{o}{.}\PY{n}{strip}\PY{p}{(}\PY{p}{)}
            \PY{c+c1}{\PYZsh{} tokenize document}
            \PY{n}{tokens} \PY{o}{=} \PY{n}{wpt}\PY{o}{.}\PY{n}{tokenize}\PY{p}{(}\PY{n}{doc}\PY{p}{)}
            \PY{c+c1}{\PYZsh{} filter stopwords out of document}
            \PY{n}{filtered\PYZus{}tokens} \PY{o}{=} \PY{p}{[}\PY{n}{token} \PY{k}{for} \PY{n}{token} \PY{o+ow}{in} \PY{n}{tokens} \PY{k}{if} \PY{n}{token} \PY{o+ow}{not} \PY{o+ow}{in} \PY{n}{stop\PYZus{}words}\PY{p}{]}
            \PY{c+c1}{\PYZsh{} re\PYZhy{}create document from filtered tokens}
            \PY{n}{doc} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ }\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n}{filtered\PYZus{}tokens}\PY{p}{)}
            \PY{k}{return} \PY{n}{doc}
        
        \PY{n}{normalize\PYZus{}corpus} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{vectorize}\PY{p}{(}\PY{n}{normalize\PYZus{}document}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{n}{norm\PYZus{}corpus} \PY{o}{=} \PY{n}{normalize\PYZus{}corpus}\PY{p}{(}\PY{n}{corpus}\PY{p}{)}
        \PY{n}{norm\PYZus{}corpus}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}4}]:} array(['sky blue beautiful', 'love blue beautiful sky',
               'quick brown fox jumps lazy dog', 'brown fox quick blue dog lazy',
               'sky blue sky beautiful today', 'dog lazy brown fox quick'],
              dtype='<U30')
\end{Verbatim}
            
    \hypertarget{bag-of-words-model}{%
\section{1. Bag of Words Model}\label{bag-of-words-model}}

    This is perhaps the most simple vector space representational model for
unstructured text. A vector space model is simply a mathematical model
to represent unstructured text (or any other data) as numeric vectors,
such that each dimension of the vector is a specific feature\attribute.
The bag of words model represents each text document as a numeric vector
where each dimension is a specific word from the corpus and the value
could be its frequency in the document, occurrence (denoted by 1 or 0)
or even weighted values. The model's name is such because each document
is represented literally as a `bag' of its own words, disregarding word
orders, sequences and grammar.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{feature\PYZus{}extraction}\PY{n+nn}{.}\PY{n+nn}{text} \PY{k}{import} \PY{n}{CountVectorizer}
        
        \PY{n}{cv} \PY{o}{=} \PY{n}{CountVectorizer}\PY{p}{(}\PY{n}{min\PYZus{}df}\PY{o}{=}\PY{l+m+mf}{0.}\PY{p}{,} \PY{n}{max\PYZus{}df}\PY{o}{=}\PY{l+m+mf}{1.}\PY{p}{)}
        \PY{n}{cv\PYZus{}matrix} \PY{o}{=} \PY{n}{cv}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{norm\PYZus{}corpus}\PY{p}{)}
        \PY{n}{cv\PYZus{}matrix} \PY{o}{=} \PY{n}{cv\PYZus{}matrix}\PY{o}{.}\PY{n}{toarray}\PY{p}{(}\PY{p}{)}
        \PY{n}{cv\PYZus{}matrix}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}5}]:} array([[1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0],
               [1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0],
               [0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0],
               [0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0],
               [1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 1],
               [0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0]], dtype=int64)
\end{Verbatim}
            
    Thus you can see that our documents have been converted into numeric
vectors such that each document is represented by one vector (row) in
the above feature matrix. The following code will help represent this in
a more easy to understand format.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{c+c1}{\PYZsh{} get all unique words in the corpus}
        \PY{n}{vocab} \PY{o}{=} \PY{n}{cv}\PY{o}{.}\PY{n}{get\PYZus{}feature\PYZus{}names}\PY{p}{(}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} show document feature vectors}
        \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{cv\PYZus{}matrix}\PY{p}{,} \PY{n}{columns}\PY{o}{=}\PY{n}{vocab}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}6}]:}    beautiful  blue  brown  dog  fox  jumps  lazy  love  quick  sky  today
        0          1     1      0    0    0      0     0     0      0    1      0
        1          1     1      0    0    0      0     0     1      0    1      0
        2          0     0      1    1    1      1     1     0      1    0      0
        3          0     1      1    1    1      0     1     0      1    0      0
        4          1     1      0    0    0      0     0     0      0    2      1
        5          0     0      1    1    1      0     1     0      1    0      0
\end{Verbatim}
            
    This should make things more clearer! You can clearly see that each
column or dimension in the feature vectors represents a word from the
corpus and each row represents one of our documents. The value in any
cell, represents the number of times that word (represented by column)
occurs in the specific document (represented by row). Hence if a corpus
of documents consists of N unique words across all the documents, we
would have an N-dimensional vector for each of the documents.

    This should make things more clearer! You can clearly see that each
column or dimension in the feature vectors represents a word from the
corpus and each row represents one of our documents. The value in any
cell, represents the number of times that word (represented by column)
occurs in the specific document (represented by row). Hence if a corpus
of documents consists of N unique words across all the documents, we
would have an N-dimensional vector for each of the documents.

    \hypertarget{bag-of-n-grams-model}{%
\section{2. Bag of N-Grams Model}\label{bag-of-n-grams-model}}

    A word is just a single token, often known as a unigram or 1-gram. We
already know that the Bag of Words model doesn't consider order of
words. But what if we also wanted to take into account phrases or
collection of words which occur in a sequence? N-grams help us achieve
that. An N-gram is basically a collection of word tokens from a text
document such that these tokens are contiguous and occur in a sequence.
Bi-grams indicate n-grams of order 2 (two words), Tri-grams indicate
n-grams of order 3 (three words), and so on. The Bag of N-Grams model is
hence just an extension of the Bag of Words model so we can also
leverage N-gram based features. The following example depicts bi-gram
based features in each document feature vector.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{c+c1}{\PYZsh{} you can set the n\PYZhy{}gram range to 1,2 to get unigrams as well as bigrams}
        \PY{n}{bv} \PY{o}{=} \PY{n}{CountVectorizer}\PY{p}{(}\PY{n}{ngram\PYZus{}range}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
        \PY{n}{bv\PYZus{}matrix} \PY{o}{=} \PY{n}{bv}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{norm\PYZus{}corpus}\PY{p}{)}
        
        \PY{n}{bv\PYZus{}matrix} \PY{o}{=} \PY{n}{bv\PYZus{}matrix}\PY{o}{.}\PY{n}{toarray}\PY{p}{(}\PY{p}{)}
        \PY{n}{vocab} \PY{o}{=} \PY{n}{bv}\PY{o}{.}\PY{n}{get\PYZus{}feature\PYZus{}names}\PY{p}{(}\PY{p}{)}
        \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{bv\PYZus{}matrix}\PY{p}{,} \PY{n}{columns}\PY{o}{=}\PY{n}{vocab}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}7}]:}    beautiful sky  beautiful today  blue beautiful  blue dog  blue sky  \textbackslash{}
        0              0                0               1         0         0   
        1              1                0               1         0         0   
        2              0                0               0         0         0   
        3              0                0               0         1         0   
        4              0                1               0         0         1   
        5              0                0               0         0         0   
        
           brown fox  dog lazy  fox jumps  fox quick  jumps lazy  lazy brown  \textbackslash{}
        0          0         0          0          0           0           0   
        1          0         0          0          0           0           0   
        2          1         0          1          0           1           0   
        3          1         1          0          1           0           0   
        4          0         0          0          0           0           0   
        5          1         1          0          1           0           1   
        
           lazy dog  love blue  quick blue  quick brown  sky beautiful  sky blue  
        0         0          0           0            0              0         1  
        1         0          1           0            0              0         0  
        2         1          0           0            1              0         0  
        3         0          0           1            0              0         0  
        4         0          0           0            0              1         1  
        5         0          0           0            0              0         0  
\end{Verbatim}
            
    This gives us feature vectors for our documents, where each feature
consists of a bi-gram representing a sequence of two words and values
represent how many times the bi-gram was present for our documents.

    \hypertarget{tf-idf-model}{%
\section{3. TF-IDF Model}\label{tf-idf-model}}

    There are some potential problems which might arise with the Bag of
Words model when it is used on large corpora. Since the feature vectors
are based on absolute term frequencies, there might be some terms which
occur frequently across all documents and these may tend to overshadow
other terms in the feature set. The TF-IDF model tries to combat this
issue by using a scaling or normalizing factor in its computation.
TF-IDF stands for Term Frequency-Inverse Document Frequency, which uses
a combination of two metrics in its computation, namely: term frequency
(tf) and inverse document frequency (idf). This technique was developed
for ranking results for queries in search engines and now it is an
indispensable model in the world of information retrieval and NLP.

Mathematically, we can define TF-IDF as tfidf = tf x idf, which can be
expanded further to be represented as follows.

Here, tfidf(w, D) is the TF-IDF score for word w in document D. The term
tf(w, D) represents the term frequency of the word w in document D,
which can be obtained from the Bag of Words model. The term idf(w, D) is
the inverse document frequency for the term w, which can be computed as
the log transform of the total number of documents in the corpus C
divided by the document frequency of the word w, which is basically the
frequency of documents in the corpus where the word w occurs. There are
multiple variants of this model but they all end up giving quite similar
results. Let's apply this on our corpus now!

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{feature\PYZus{}extraction}\PY{n+nn}{.}\PY{n+nn}{text} \PY{k}{import} \PY{n}{TfidfVectorizer}
        
        \PY{n}{tv} \PY{o}{=} \PY{n}{TfidfVectorizer}\PY{p}{(}\PY{n}{min\PYZus{}df}\PY{o}{=}\PY{l+m+mf}{0.}\PY{p}{,} \PY{n}{max\PYZus{}df}\PY{o}{=}\PY{l+m+mf}{1.}\PY{p}{,} \PY{n}{use\PYZus{}idf}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
        \PY{n}{tv\PYZus{}matrix} \PY{o}{=} \PY{n}{tv}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{norm\PYZus{}corpus}\PY{p}{)}
        \PY{n}{tv\PYZus{}matrix} \PY{o}{=} \PY{n}{tv\PYZus{}matrix}\PY{o}{.}\PY{n}{toarray}\PY{p}{(}\PY{p}{)}
        
        \PY{n}{vocab} \PY{o}{=} \PY{n}{tv}\PY{o}{.}\PY{n}{get\PYZus{}feature\PYZus{}names}\PY{p}{(}\PY{p}{)}
        \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{n}{tv\PYZus{}matrix}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{columns}\PY{o}{=}\PY{n}{vocab}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}8}]:}    beautiful  blue  brown   dog   fox  jumps  lazy  love  quick   sky  today
        0       0.60  0.52   0.00  0.00  0.00   0.00  0.00  0.00   0.00  0.60   0.00
        1       0.46  0.39   0.00  0.00  0.00   0.00  0.00  0.66   0.00  0.46   0.00
        2       0.00  0.00   0.38  0.38  0.38   0.54  0.38  0.00   0.38  0.00   0.00
        3       0.00  0.36   0.42  0.42  0.42   0.00  0.42  0.00   0.42  0.00   0.00
        4       0.36  0.31   0.00  0.00  0.00   0.00  0.00  0.00   0.00  0.72   0.52
        5       0.00  0.00   0.45  0.45  0.45   0.00  0.45  0.00   0.45  0.00   0.00
\end{Verbatim}
            
    The TF-IDF based feature vectors for each of our text documents show
scaled and normalized values as compared to the raw Bag of Words model
values. Interested readers who might want to dive into further details
of how the internals of this model work can refer to page 181 of Text
Analytics with Python (Springer\Apress; Dipanjan Sarkar, 2016).

    \hypertarget{section}{%
\section{===================================}\label{section}}

\hypertarget{part-b.-intermediate-materials}{%
\section{Part B. Intermediate
Materials}\label{part-b.-intermediate-materials}}

\hypertarget{section-1}{%
\section{===================================}\label{section-1}}

    There is still time left? Let's cover some more advanced clustering
techniques:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Document Clustering with Similarity Features
\item
  Topic Models
\item
  Document Similarity
\end{enumerate}

    \hypertarget{document-similarity}{%
\section{1. Document Similarity}\label{document-similarity}}

    Document similarity is the process of using a distance or similarity
based metric that can be used to identify how similar a text document is
with any other document(s) based on features extracted from the
documents like bag of words or tf-idf. Thus you can see that we can
build on top of the tf-idf based features we engineered in the previous
section and use them to generate new features which can be useful in
domains like search engines, document clustering and information
retrieval by leveraging these similarity based features.

Pairwise document similarity in a corpus involves computing document
similarity for each pair of documents in a corpus. Thus if you have C
documents in a corpus, you would end up with a C x C matrix such that
each row and column represents the similarity score for a pair of
documents, which represent the indices at the row and column,
respectively. There are several similarity and distance metrics that are
used to compute document similarity. These include cosine
distance/similarity, euclidean distance, manhattan distance, BM25
similarity, jaccard distance and so on. In our analysis, we will be
using perhaps the most popular and widely used similarity metric, cosine
similarity and compare pairwise document similarity based on their
TF-IDF feature vectors.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics}\PY{n+nn}{.}\PY{n+nn}{pairwise} \PY{k}{import} \PY{n}{cosine\PYZus{}similarity}
        
        \PY{n}{similarity\PYZus{}matrix} \PY{o}{=} \PY{n}{cosine\PYZus{}similarity}\PY{p}{(}\PY{n}{tv\PYZus{}matrix}\PY{p}{)}
        \PY{n}{similarity\PYZus{}df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{similarity\PYZus{}matrix}\PY{p}{)}
        \PY{n}{similarity\PYZus{}df}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}9}]:}           0         1         2         3         4         5
        0  1.000000  0.753128  0.000000  0.185447  0.807539  0.000000
        1  0.753128  1.000000  0.000000  0.139665  0.608181  0.000000
        2  0.000000  0.000000  1.000000  0.784362  0.000000  0.839987
        3  0.185447  0.139665  0.784362  1.000000  0.109653  0.933779
        4  0.807539  0.608181  0.000000  0.109653  1.000000  0.000000
        5  0.000000  0.000000  0.839987  0.933779  0.000000  1.000000
\end{Verbatim}
            
    Cosine similarity basically gives us a metric representing the cosine of
the angle between the feature vector representations of two text
documents. Lower the angle between the documents, the closer and more
similar they are as depicted in the following figure.

Cosine similarity depictions for text document feature vectors Looking
closely at the similarity matrix clearly tells us that documents (0, 1
and 6), (2, 5 and 7) are very similar to one another and documents 3 and
4 are slightly similar to each other but the magnitude is not very
strong, however still stronger than the other documents. This must
indicate these similar documents have some similar features. This is a
perfect example of grouping or clustering that can be solved by
unsupervised learning especially when you are dealing with huge corpora
of millions of text documents.

    \hypertarget{document-clustering-with-similarity-features}{%
\section{2. Document Clustering with Similarity
Features}\label{document-clustering-with-similarity-features}}

    Clustering leverages unsupervised learning to group data points
(documents in this scenario) into groups or clusters. We will be
leveraging an unsupervised hierarchical clustering algorithm here to try
and group similar documents from our toy corpus together by leveraging
the document similarity features we generated earlier. There are two
types of hierarchical clustering algorithms namely, agglomerative and
divisive methods. We will be using a agglomerative clustering algorithm,
which is hierarchical clustering using a bottom up approach i.e.~each
observation or document starts in its own cluster and clusters are
successively merged together using a distance metric which measures
distances between data points and a linkage merge criterion. A sample
depiction is shown in the following figure.

The selection of the linkage criterion governs the merge strategy. Some
examples of linkage criteria are Ward, Complete linkage, Average linkage
and so on. This criterion is very useful for choosing the pair of
clusters (individual documents at the lowest step and clusters in higher
steps) to merge at each step is based on the optimal value of an
objective function. We choose the Ward's minimum variance method as our
linkage criterion to minimize total within-cluster variance. Hence, at
each step, we find the pair of clusters that leads to minimum increase
in total within-cluster variance after merging. Since we already have
our similarity features, let's build out the linkage matrix on our
sample documents.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{k+kn}{from} \PY{n+nn}{scipy}\PY{n+nn}{.}\PY{n+nn}{cluster}\PY{n+nn}{.}\PY{n+nn}{hierarchy} \PY{k}{import} \PY{n}{dendrogram}\PY{p}{,} \PY{n}{linkage}
         
         \PY{n}{Z} \PY{o}{=} \PY{n}{linkage}\PY{p}{(}\PY{n}{similarity\PYZus{}matrix}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ward}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{Z}\PY{p}{,} \PY{n}{columns}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Document}\PY{l+s+s1}{\PYZbs{}}\PY{l+s+s1}{Cluster 1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Document}\PY{l+s+s1}{\PYZbs{}}\PY{l+s+s1}{Cluster 2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                                  \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Distance}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Cluster Size}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{object}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}10}]:}   Document\textbackslash{}Cluster 1 Document\textbackslash{}Cluster 2  Distance Cluster Size
         0                  2                  5  0.271171            2
         1                  0                  4  0.317548            2
         2                  3                  6  0.373037            3
         3                  1                  7  0.531801            3
         4                  8                  9   3.44916            6
\end{Verbatim}
            
    If you closely look at the linkage matrix, you can see that each step
(row) of the linkage matrix tells us which data points (or clusters)
were merged together. If you have n data points, the linkage matrix, Z
will be having a shape of (n --- 1) x 4 where Z{[}i{]} will tell us
which clusters were merged at step i. Each row has four elements, the
first two elements are either data point identifiers or cluster labels
(in the later parts of the matrix once multiple data points are merged),
the third element is the cluster distance between the first two elements
(either data points or clusters), and the last element is the total
number of elements\data points in the cluster once the merge is
complete. We recommend you refer to the scipy documentation, which
explains this in detail.

Let's now visualize this matrix as a dendrogram to understand the
elements better!

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Hierarchical Clustering Dendrogram}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Data point}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Distance}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{dendrogram}\PY{p}{(}\PY{n}{Z}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{axhline}\PY{p}{(}\PY{n}{y}\PY{o}{=}\PY{l+m+mf}{1.0}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{k}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{ls}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{lw}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}11}]:} <matplotlib.lines.Line2D at 0x1a1acfca90>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_37_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{topic-models}{%
\section{Topic Models}\label{topic-models}}

We can also use some summarization techniques to extract topic or
concept based features from text documents. The idea of topic models
revolves around the process of extracting key themes or concepts from a
corpus of documents which are represented as topics. Each topic can be
represented as a bag or collection of words/terms from the document
corpus. Together, these terms signify a specific topic, theme or a
concept and each topic can be easily distinguished from other topics by
virtue of the semantic meaning conveyed by these terms. However often
you do end up with overlapping topics based on the data. These concepts
can range from simple facts and statements to opinions and outlook.
Topic models are extremely useful in summarizing large corpus of text
documents to extract and depict key concepts. They are also useful in
extracting features from text data that capture latent patterns in the
data.

There are various techniques for topic modeling and most of them involve
some form of matrix decomposition. Some techniques like Latent Semantic
Indexing (LSI) use matrix decomposition operations, more specifically
Singular Valued Decomposition. We will be using another technique is
Latent Dirichlet Allocation (LDA), which uses a generative probabilistic
model where each document consists of a combination of several topics
and each term or word can be assigned to a specific topic. This is
similar to pLSI based model (probabilistic LSI). Each latent topic
contains a Dirichlet prior over them in the case of LDA.

The math behind in this technique is pretty involved, so I will try to
summarize it without boring you with a lot of details. I recommend
readers to go through this excellent talk by Christine Doig.

The black box in the above figure represents the core algorithm that
makes use of the previously mentioned parameters to extract K topics
from M documents. The following steps give a simplistic explanation of
what happens in the algorithm behind the scenes.

Once this runs for several iterations, we should have topic mixtures for
each document and then generate the constituents of each topic from the
terms that point to that topic. Frameworks like gensim or scikit-learn
enable us to leverage the LDA model for generating topics.

For the purpose of feature engineering which is the intent of this
article, you need to remember that when LDA is applied on a
document-term matrix (TF-IDF or Bag of Words feature matrix), it gets
decomposed into two main components.

A document-topic matrix, which would be the feature matrix we are
looking for. A topic-term matrix, which helps us in looking at potential
topics in the corpus. Let's leverage scikit-learn to get the
document-topic matrix as follows.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{decomposition} \PY{k}{import} \PY{n}{LatentDirichletAllocation}
         
         \PY{n}{lda} \PY{o}{=} \PY{n}{LatentDirichletAllocation}\PY{p}{(}\PY{n}{n\PYZus{}topics}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,} \PY{n}{max\PYZus{}iter}\PY{o}{=}\PY{l+m+mi}{10000}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
         \PY{n}{dt\PYZus{}matrix} \PY{o}{=} \PY{n}{lda}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{cv\PYZus{}matrix}\PY{p}{)}
         \PY{n}{features} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{dt\PYZus{}matrix}\PY{p}{,} \PY{n}{columns}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{T1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{T2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{T3}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         \PY{n}{features}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
/Users/james/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online\_lda.py:294: DeprecationWarning: n\_topics has been renamed to n\_components in version 0.19 and will be removed in 0.21
  DeprecationWarning)
/Users/james/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online\_lda.py:536: DeprecationWarning: The default value for 'learning\_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.
  DeprecationWarning)

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}12}]:}          T1        T2        T3
         0  0.831570  0.084281  0.084149
         1  0.864945  0.067312  0.067743
         2  0.047801  0.903651  0.048548
         3  0.055404  0.896033  0.048563
         4  0.887660  0.055993  0.056347
         5  0.055710  0.887959  0.056331
\end{Verbatim}
            
    You can clearly see which documents contribute the most to which of the
three topics in the above output. You can view the topics and their main
constituents as follows.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}13}]:} \PY{n}{tt\PYZus{}matrix} \PY{o}{=} \PY{n}{lda}\PY{o}{.}\PY{n}{components\PYZus{}}
         \PY{k}{for} \PY{n}{topic\PYZus{}weights} \PY{o+ow}{in} \PY{n}{tt\PYZus{}matrix}\PY{p}{:}
             \PY{n}{topic} \PY{o}{=} \PY{p}{[}\PY{p}{(}\PY{n}{token}\PY{p}{,} \PY{n}{weight}\PY{p}{)} \PY{k}{for} \PY{n}{token}\PY{p}{,} \PY{n}{weight} \PY{o+ow}{in} \PY{n+nb}{zip}\PY{p}{(}\PY{n}{vocab}\PY{p}{,} \PY{n}{topic\PYZus{}weights}\PY{p}{)}\PY{p}{]}
             \PY{n}{topic} \PY{o}{=} \PY{n+nb}{sorted}\PY{p}{(}\PY{n}{topic}\PY{p}{,} \PY{n}{key}\PY{o}{=}\PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{o}{\PYZhy{}}\PY{n}{x}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
             \PY{n}{topic} \PY{o}{=} \PY{p}{[}\PY{n}{item} \PY{k}{for} \PY{n}{item} \PY{o+ow}{in} \PY{n}{topic} \PY{k}{if} \PY{n}{item}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{\PYZgt{}} \PY{l+m+mf}{0.6}\PY{p}{]}
             \PY{n+nb}{print}\PY{p}{(}\PY{n}{topic}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
[('sky', 4.330354268539671), ('blue', 3.3755375835723727), ('beautiful', 3.3301184041393532), ('today', 1.3307001091581214), ('love', 1.3299749582580207)]

[('brown', 3.3302369156308287), ('dog', 3.3302369156308287), ('fox', 3.3302369156308287), ('lazy', 3.3302369156308287), ('quick', 3.3302369156308287), ('jumps', 1.330279231916799), ('blue', 1.285679256293642)]

[]


    \end{Verbatim}

    \hypertarget{document-clustering-with-topic-model-features}{%
\section{Document Clustering with Topic Model
Features}\label{document-clustering-with-topic-model-features}}

We used our Bag of Words model based features to build out topic model
based features using LDA. We can now actually leverage the document term
matrix we obtained and use an unsupervised clustering algorithm to try
and group our documents similar to what we did earlier with our
similarity features.

We will use a very popular partition based clustering method this time,
K-means clustering to cluster or group these documents based on their
topic model feature representations. In K-means clustering, we have an
input parameter k, which specifies the number of clusters it will output
using the document features. This clustering method is a centroid based
clustering method, where it tries to cluster these documents into
clusters of equal variance. It tries to create these clusters by
minimizing the within-cluster sum of squares measure, also known as
inertia. There are multiple ways to select the optimal value of k like
using the Sum of Squared Errors metric, Silhouette Coefficients and the
Elbow method.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{cluster} \PY{k}{import} \PY{n}{KMeans}
         
         \PY{n}{km} \PY{o}{=} \PY{n}{KMeans}\PY{p}{(}\PY{n}{n\PYZus{}clusters}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
         \PY{n}{km}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{features}\PY{p}{)}
         \PY{n}{cluster\PYZus{}labels} \PY{o}{=} \PY{n}{km}\PY{o}{.}\PY{n}{labels\PYZus{}}
         \PY{n}{cluster\PYZus{}labels} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{cluster\PYZus{}labels}\PY{p}{,} \PY{n}{columns}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ClusterLabel}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         \PY{n}{pd}\PY{o}{.}\PY{n}{concat}\PY{p}{(}\PY{p}{[}\PY{n}{corpus\PYZus{}df}\PY{p}{,} \PY{n}{cluster\PYZus{}labels}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}14}]:}                                             Document Category  ClusterLabel
         0                     The sky is blue and beautiful.  weather             2
         1                  Love this blue and beautiful sky!  weather             0
         2       The quick brown fox jumps over the lazy dog.  animals             1
         3   The brown fox is quick and the blue dog is lazy!  animals             1
         4  The sky is very blue and the sky is very beaut{\ldots}  weather             0
         5        The dog is lazy but the brown fox is quick!  animals             1
\end{Verbatim}
            
    \hypertarget{section}{%
\section{===================================}\label{section}}

\hypertarget{part-c.-advanced-materials}{%
\section{Part C. Advanced Materials}\label{part-c.-advanced-materials}}

\hypertarget{section-1}{%
\section{===================================}\label{section-1}}

    Done and have more time? Help your classmates with completing their
exercise or try the next tutorial on word embeddings as features.

https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa

    \hypertarget{shortcomings-of-traditional-models}{%
\section{Shortcomings of traditional
models:}\label{shortcomings-of-traditional-models}}

Traditional (count-based) feature engineering strategies for textual
data involve models belonging to a family of models popularly known as
the Bag of Words model. This includes term frequencies, TF-IDF (term
frequency-inverse document frequency), N-grams and so on. While they are
effective methods for extracting features from text, due to the inherent
nature of the model being just a bag of unstructured words, we lose
additional information like the semantics, structure, sequence and
context around nearby words in each text document. This forms as enough
motivation for us to explore more sophisticated models which can capture
this information and give us features which are vector representation of
words, popularly known as embeddings.

\hypertarget{the-need-for-word-embeddings}{%
\section{The need for word
embeddings:}\label{the-need-for-word-embeddings}}

While this does make some sense, why should we be motivated enough to
learn and build these word embeddings? With regard to speech or image
recognition systems, all the information is already present in the form
of rich dense feature vectors embedded in high-dimensional datasets like
audio spectrograms and image pixel intensities. However when it comes to
raw text data, especially count based models like Bag of Words, we are
dealing with individual words which may have their own identifiers and
do not capture the semantic relationship amongst words. This leads to
huge sparse word vectors for textual data and thus if we do not have
enough data, we may end up getting poor models or even overfitting the
data due to the curse of dimensionality.

To overcome the shortcomings of losing out semantics and feature
sparsity in bag of words model based features, we need to make use of
Vector Space Models (VSMs) in such a way that we can embed word vectors
in this continuous vector space based on semantic and contextual
similarity. In fact the distributional hypothesis in the field of
distributional semantics tells us that words which occur and are used in
the same context, are semantically similar to one another and have
similar meanings. In simple terms, `a word is characterized by the
company it keeps'. One of the famous papers talking about these semantic
word vectors and various types in detail is `Don't count, predict! A
systematic comparison of context-counting vs.~context-predicting
semantic vectors' by Baroni et al.~We won't go into extensive depth but
in short, there are two main types of methods for contextual word
vectors. Count-based methods like Latent Semantic Analysis (LSA) which
can be used to compute some statistical measures of how often words
occur with their neighboring words in a corpus and then building out
dense word vectors for each word from these measures. Predictive methods
like Neural Network based language models try to predict words from its
neighboring words looking at word sequences in the corpus and in the
process it learns distributed representations giving us dense word
embeddings. We will be focusing on these predictive methods in this
article.


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
